2025-12-28 08:49:52.852 31539-31539 VRI[MainActivity]       com.lmstudio.mobile                  D  WindowInsets changed: 1080x2400 statusBars:[0,63,0,0] navigationBars:[0,0,0,63] mandatorySystemGestures:[0,63,0,84] 
2025-12-28 08:49:52.855   817-852   CoreBackPreview         system_server                        D  Window{4f83e22 u0 com.lmstudio.mobile/com.lmstudio.mobile.MainActivity}: Setting back callback OnBackInvokedCallbackInfo{mCallback=android.window.IOnBackInvokedCallback$Stub$Proxy@2801a6e, mPriority=0, mIsAnimationCallback=false, mOverrideBehavior=0}
2025-12-28 08:49:53.408 31539-31539 LlamaCppEngine          com.lmstudio.mobile                  I  Native library 'llama_jni' loaded successfully
2025-12-28 08:49:53.410 31539-31539 AppStartup              com.lmstudio.mobile                  D  LLMModule: providing InferenceManager...
2025-12-28 08:49:53.413 31539-31539 InferenceManager        com.lmstudio.mobile                  D  InferenceManager initialized
2025-12-28 08:49:53.420 31539-31539 ChatViewModel           com.lmstudio.mobile                  D  ChatViewModel initialized
2025-12-28 08:49:53.421 31539-31539 ChatViewModel           com.lmstudio.mobile                  D  InferenceState changed to: IDLE
2025-12-28 08:49:53.622   817-856   ActivityTaskManager     system_server                        I  Displayed com.lmstudio.mobile/.MainActivity for user 0: +2s506ms
2025-12-28 08:50:09.470 31539-31539 InferenceManager        com.lmstudio.mobile                  I  loadModel START: path=/storage/emulated/0/Android/data/com.lmstudio.mobile/files/models/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf, nThreads=1, nGpuLayers=0, contextSize=2048
2025-12-28 08:50:09.470 31539-31539 ChatViewModel           com.lmstudio.mobile                  D  InferenceState changed to: LOADING
2025-12-28 08:50:09.472 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  I  loadModel: path=/storage/emulated/0/Android/data/com.lmstudio.mobile/files/models/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf, threads=1, gpuLayers=0, contextSize=2048
2025-12-28 08:50:09.480 31539-31565 LlamaJNI                com.lmstudio.mobile                  I  Native loading: /storage/emulated/0/Android/data/com.lmstudio.mobile/files/models/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf
2025-12-28 08:50:15.444 31539-31565 LlamaJNI                com.lmstudio.mobile                  I  Model loaded pointer: 0x79fa41231c20
2025-12-28 08:50:15.447 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  I  loadModel: SUCCESS
2025-12-28 08:50:15.461 31539-31539 InferenceManager        com.lmstudio.mobile                  D  getModelInfo: name=DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf, contextLength=2048
2025-12-28 08:50:15.461 31539-31539 InferenceManager        com.lmstudio.mobile                  I  loadModel SUCCESS: DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf
2025-12-28 08:50:15.461 31539-31539 ChatViewModel           com.lmstudio.mobile                  D  InferenceState changed to: READY
2025-12-28 08:50:18.177   817-851   CoreBackPreview         system_server                        D  Window{4f83e22 u0 com.lmstudio.mobile/com.lmstudio.mobile.MainActivity}: Setting back callback OnBackInvokedCallbackInfo{mCallback=android.window.IOnBackInvokedCallback$Stub$Proxy@b2be9d2, mPriority=0, mIsAnimationCallback=true, mOverrideBehavior=0}
2025-12-28 08:50:25.418 31539-31539 InferenceManager        com.lmstudio.mobile                  D  isModelLoaded check: true
2025-12-28 08:50:25.418 31539-31539 ChatViewModel           com.lmstudio.mobile                  I  sendMessage called: length=17, modelLoaded=true
2025-12-28 08:50:25.418 31539-31539 InferenceManager        com.lmstudio.mobile                  D  isModelLoaded check: true
2025-12-28 08:50:25.462 31539-31539 ChatViewModel           com.lmstudio.mobile                  D  Starting generation with prompt from 1 messages
2025-12-28 08:50:25.464 31539-31539 InferenceManager        com.lmstudio.mobile                  I  generateCompletion START: chatId=f10faf1f-89d4-4c81-a16a-29cf2bc9b45f, messageCount=1, state=READY
2025-12-28 08:50:25.465 31539-31539 ChatViewModel           com.lmstudio.mobile                  D  InferenceState changed to: GENERATING
2025-12-28 08:50:25.466 31539-31539 InferenceManager        com.lmstudio.mobile                  D  generateCompletion: state set to GENERATING, chatId=f10faf1f-89d4-4c81-a16a-29cf2bc9b45f, assistantMessageId=ae8ea348-0cd4-4894-a39b-83a8d98acfce
2025-12-28 08:50:25.466 31539-31539 InferenceManager        com.lmstudio.mobile                  D  getModelInfo: name=DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf, contextLength=2048
2025-12-28 08:50:25.466 31539-31539 InferenceManager        com.lmstudio.mobile                  D  buildPrompt: detecting template for model 'deepseek-r1-distill-qwen-1.5b-q3_k_m.gguf' from messages: [USER:capital of russia]
2025-12-28 08:50:25.467 31539-31539 InferenceManager        com.lmstudio.mobile                  D  buildPrompt: using template DEEPSEEK
2025-12-28 08:50:25.468 31539-31539 InferenceManager        com.lmstudio.mobile                  D  getModelInfo: name=DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf, contextLength=2048
2025-12-28 08:50:25.468 31539-31539 InferenceManager        com.lmstudio.mobile                  D  getModelInfo: name=DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf, contextLength=2048
2025-12-28 08:50:25.468 31539-31539 InferenceManager        com.lmstudio.mobile                  D  generateCompletion prompt built (length=79)
2025-12-28 08:50:25.500 31539-31567 LlamaJNI                com.lmstudio.mobile                  I  Native stop flag reset
2025-12-28 08:50:25.500 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  I  generateResponse: START - prompt length=79
2025-12-28 08:50:25.514 31539-31567 LlamaJNI                com.lmstudio.mobile                  I  Context reset: KV cache cleared, sampler reset, n_past=0, piece_buffer cleared
2025-12-28 08:50:25.514 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  D  generateResponse: context reset
2025-12-28 08:50:25.554   817-1592  CoreBackPreview         system_server                        D  Window{4f83e22 u0 com.lmstudio.mobile/com.lmstudio.mobile.MainActivity}: Setting back callback OnBackInvokedCallbackInfo{mCallback=android.window.IOnBackInvokedCallback$Stub$Proxy@f99ec6f, mPriority=0, mIsAnimationCallback=false, mOverrideBehavior=0}
2025-12-28 08:53:03.977 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[0]='<'
2025-12-28 08:53:03.994 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '<'
2025-12-28 08:53:11.501 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[1]='|'
2025-12-28 08:53:11.502 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:53:19.079 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[2]='im'
2025-12-28 08:53:19.096 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'im'
2025-12-28 08:53:26.592 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[3]='_start'
2025-12-28 08:53:26.600 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '_start'
2025-12-28 08:53:33.942 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[4]='|'
2025-12-28 08:53:33.948 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:53:41.435 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[5]='>'
2025-12-28 08:53:41.436 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '>'
2025-12-28 08:53:48.940 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[6]='user'
2025-12-28 08:53:48.948 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'user'
2025-12-28 08:53:56.372 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[7]='
                                                                                                    '
2025-12-28 08:53:56.382 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '
                                                                                                    '
2025-12-28 08:54:04.086 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[8]='<'
2025-12-28 08:54:04.092 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '<'
2025-12-28 08:54:11.832 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[9]='|'
2025-12-28 08:54:11.834 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:54:19.504 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[10]='im'
2025-12-28 08:54:19.511 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'im'
2025-12-28 08:54:27.265 31539-31567 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[11]='_end'
2025-12-28 08:54:27.270 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '_end'
2025-12-28 08:54:34.898 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[12]='|'
2025-12-28 08:54:34.912 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:54:42.538 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[13]='>
                                                                                                    '
2025-12-28 08:54:42.542 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '>
                                                                                                    '
2025-12-28 08:54:50.484 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[14]='<'
2025-12-28 08:54:50.487 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '<'
2025-12-28 08:54:58.151 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[15]='|'
2025-12-28 08:54:58.155 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:55:05.677 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[16]='im'
2025-12-28 08:55:05.686 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'im'
2025-12-28 08:55:13.076 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[17]='_start'
2025-12-28 08:55:13.080 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '_start'
2025-12-28 08:55:20.459 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[18]='|'
2025-12-28 08:55:20.466 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:55:28.185 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[19]='>'
2025-12-28 08:55:28.186 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '>'
2025-12-28 08:55:36.112 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[20]='assistant'
2025-12-28 08:55:36.127 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'assistant'
2025-12-28 08:55:44.022 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[21]='
                                                                                                    '
2025-12-28 08:55:44.045 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '
                                                                                                    '
2025-12-28 08:55:51.572 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[22]='<'
2025-12-28 08:55:51.582 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '<'
2025-12-28 08:55:59.520 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[23]='|'
2025-12-28 08:55:59.544 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:56:07.597 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[24]='thought'
2025-12-28 08:56:07.630 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'thought'
2025-12-28 08:56:15.340 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[25]='|'
2025-12-28 08:56:15.342 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:56:23.227 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[26]='>
                                                                                                    '
2025-12-28 08:56:23.234 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '>
                                                                                                    '
2025-12-28 08:56:31.321 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[27]='<'
2025-12-28 08:56:31.323 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '<'
2025-12-28 08:56:38.895 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[28]='|'
2025-12-28 08:56:38.911 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:56:46.469 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[29]='im'
2025-12-28 08:56:46.472 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'im'
2025-12-28 08:56:54.319 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[30]='_start'
2025-12-28 08:56:54.320 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '_start'
2025-12-28 08:57:02.005 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[31]='|'
2025-12-28 08:57:02.011 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:57:09.699 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[32]='>'
2025-12-28 08:57:09.711 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '>'
2025-12-28 08:57:17.458 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[33]='user'
2025-12-28 08:57:17.464 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'user'
2025-12-28 08:57:24.907 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[34]='
                                                                                                    '
2025-12-28 08:57:24.917 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '
                                                                                                    '
2025-12-28 08:57:32.583 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[35]='<'
2025-12-28 08:57:32.611 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '<'
2025-12-28 08:57:40.689 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[36]='|'
2025-12-28 08:57:40.711 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:57:48.615 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[37]='im'
2025-12-28 08:57:48.619 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'im'
2025-12-28 08:57:56.504 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[38]='_end'
2025-12-28 08:57:56.513 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '_end'
2025-12-28 08:58:03.995 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[39]='|'
2025-12-28 08:58:04.011 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:58:11.637 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[40]='>
                                                                                                    '
2025-12-28 08:58:11.649 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '>
                                                                                                    '
2025-12-28 08:58:19.432 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[41]='<'
2025-12-28 08:58:19.445 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '<'
2025-12-28 08:58:26.986 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[42]='|'
2025-12-28 08:58:26.995 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:58:35.125 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[43]='im'
2025-12-28 08:58:35.129 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'im'
2025-12-28 08:58:42.620 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[44]='_start'
2025-12-28 08:58:42.622 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '_start'
2025-12-28 08:58:50.126 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[45]='|'
2025-12-28 08:58:50.132 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:58:58.042 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[46]='>'
2025-12-28 08:58:58.049 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '>'
2025-12-28 08:59:06.141 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[47]='assistant'
2025-12-28 08:59:06.147 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'assistant'
2025-12-28 08:59:13.895 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[48]='
                                                                                                    '
2025-12-28 08:59:13.897 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '
                                                                                                    '
2025-12-28 08:59:21.749 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[49]='<'
2025-12-28 08:59:21.752 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '<'
2025-12-28 08:59:29.793 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[50]='|'
2025-12-28 08:59:29.811 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:59:37.889 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[51]='thought'
2025-12-28 08:59:37.911 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: 'thought'
2025-12-28 08:59:45.774 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  V  generateResponse: token[52]='|'
2025-12-28 08:59:45.780 31539-31539 InferenceManager        com.lmstudio.mobile                  V  generateCompletion token: '|'
2025-12-28 08:59:52.901 31539-31539 InferenceManager        com.lmstudio.mobile                  I  stopGeneration called (sending cancel signal to engine)
2025-12-28 08:59:52.901 31539-31539 LlamaCppEngine          com.lmstudio.mobile                  I  stopGeneration called
2025-12-28 08:59:52.926 31539-31539 LlamaJNI                com.lmstudio.mobile                  I  Native stop flag set
2025-12-28 08:59:53.886 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  I  generateResponse: LOOP FINISHED - tokens=53
2025-12-28 08:59:53.886 31539-31565 LlamaCppEngine          com.lmstudio.mobile                  I  generateResponse: FINISHED - total tokens=53
2025-12-28 08:59:53.888 31539-31539 InferenceManager        com.lmstudio.mobile                  I  generateCompletion COMPLETE: finalContentLength=122
2025-12-28 08:59:53.888 31539-31539 ChatViewModel           com.lmstudio.mobile                  D  InferenceState changed to: READY
2025-12-28 08:59:53.894 31539-31539 ChatViewModel           com.lmstudio.mobile                  I  Generation completed, tokens received: 122
2025-12-28 08:59:53.909 31539-31539 ChatViewModel           com.lmstudio.mobile                  D  Saved assistant message to database: id=ae8ea348-0cd4-4894-a39b-83a8d98acfce
2025-12-28 08:59:54.403 31539-31539 InferenceManager        com.lmstudio.mobile                  D  generateCompletion: clearing streaming state (delayed)
